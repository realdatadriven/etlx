[{"content":"1. Introduction ETLX introduces an innovative, flexible way to handle ETL processes, data quality validation, and report automation. It empowers users with:\nMarkdown-driven Configuration: Define your workflows in a structured, human-readable format. DuckDB Integration: Utilize DuckDB‚Äôs capabilities to work with diverse data sources, including databases, object storage, and APIs. Extensibility: Easily adapt to new use cases with modular configurations. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/introduction/","title":"Introduction"},{"content":"Installation Option 1: Precompiled Binaries Precompiled binaries for Linux, macOS, and Windows are available on the releases page. Download the appropriate binary for your system and make it executable:\n1 2 3 # Example for Linux or macOS chmod +x etlx ./etlx --help Option 2: Install via Go (as a library) 1 2 # Install ETLX go get github.com/realdatadriven/etlx Option 3: Clone Repo 1 2 git clone https://github.com/realdatadriven/etlx.git cd etlx And then:\n1 go run cmd/main.go --config etl_config.md --date 2023-10-31 On Windows you may have build issues; in that case using the latest libduckdb from duckdb/releases and building with -tags=duckdb_use_lib may help:\n1 CGO_ENABLED=1 CGO_LDFLAGS=\u0026#34;-L/path/to/libs\u0026#34; go run -tags=duckdb_use_lib main.go --config etl_config.md --date 2023-10-31 ","description":null,"permalink":"https://realdatadriven.github.io/etlx/quick-start/installation/","title":"Installation"},{"content":"Running ETLX The binary supports the following flags:\n--config: Path to the Markdown configuration file. (Default: config.md) --date: Reference date for the ETL process in YYYY-MM-DD format. (Default: yesterday\u0026rsquo;s date) --only: Comma-separated list of keys to run. --skip: Comma-separated list of keys to skip. --steps: Steps to run within the ETL process (extract, transform, load). --file: Path to a specific file to extract data from. Typically used with the --only flag. --clean: Execute clean_sql on items (conditional based on --only and --skip). --drop: Execute drop_sql on items (conditional based on --only and --skip). --rows: Retrieve the number of rows in the target table(s). 1 etlx --config etl_config.md --date 2023-10-31 --only sales --steps extract,load üê≥ Running ETLX with Docker You can run etlx directly from Docker without installing Go or building locally.\nBuild the Image Clone the repo and build:\n1 docker build -t etlx:latest . Or pull the prebuilt image (when published):\n1 docker pull docker.io/realdatadriven/etlx:latest Running Commands The image behaves exactly like the CLI binary. For example:\n1 2 3 docker run --rm etlx:latest help docker run --rm etlx:latest version docker run --rm etlx:latest run --config /app/config.md Using a .env File If you have a .env file with environment variables, mount it into /app/.env:\n1 2 3 docker run --rm \\ -v $(pwd)/.env:/app/.env:ro \\ etlx:latest run --config /app/config.md Mounting Config Files Mount your config file into the container and reference it by path:\n1 2 3 docker run --rm \\ -v $(pwd)/config.md:/app/config.md:ro \\ etlx:latest run --config /app/config.md Database Directory etlx can attach a database directory. Mount your local ./database directory into /app/database:\n1 2 3 docker run --rm \\ -v $(pwd)/database:/app/database \\ etlx:latest run --config /app/config.md Combine All Together Mount .env, config, and database directory:\n1 2 3 4 5 docker run --rm \\ -v $(pwd)/.env:/app/.env:ro \\ -v $(pwd)/config.md:/app/config.md:ro \\ -v $(pwd)/database:/app/database \\ etlx:latest run --config /app/config.md Interactive Mode For interactive subcommands (like repl):\n1 docker run -it --rm etlx:latest repl üí° Pro Tip: Local Alias You can add an alias so Docker feels like the native binary:\n1 alias etlx=\u0026#34;docker run --rm -v $(pwd):/app etlx:latest\u0026#34; Now you can just run:\n1 2 etlx help etlx run --config /app/config.md How It Works Create a Markdown file with the ETL process configuration. For example, see the example use case in the examples section.\n","description":null,"permalink":"https://realdatadriven.github.io/etlx/quick-start/running/","title":"Running ETLX"},{"content":"Features Markdown-based configurations for easy readability. Extensive DuckDB support for various data sources and formats. Modular design for reusability and clarity. Built-in error handling and validation. Export functionality for reports, templates, and data lakes. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/features/","title":"Features"},{"content":"ETL Defines the overall ETL process. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # ETL ```yaml metadata name: Daily_ETL description: \u0026#39;Daily extraction at 5 AM\u0026#39; database: analytics_db connection: \u0026#39;postgres:user=@PGUSER password=@PGPASSWORD dbname=analytics_db host=localhost port=5432 sslmode=disable\u0026#39; ``` ## sales_data ```yaml metadata name: SalesData description: \u0026#39;Daily Sales Data\u0026#39; load_conn: \u0026#39;duckdb:\u0026#39; load_before_sql: - load_extentions - conn load_validation: # Validation is performed during the load phase using YAML - type: throw_if_empty sql: validate_data_not_empty msg: \u0026#39;No data extracted for the given date!\u0026#39; - type: throw_if_not_empty sql: validate_data_duplicates msg: \u0026#39;Duplicate data detected!\u0026#39; load_sql: load_sales load_after_sql: detaches ``` 1. ETL Process Starts Begin with the \u0026ldquo;ETL\u0026rdquo; key; Extract metadata, specifically: \u0026ldquo;connection\u0026rdquo;: Main connection to the destination database. \u0026ldquo;description\u0026rdquo;: For logging the start and end time of the ETL process. 2. Loop through Level 2 key in under \u0026ldquo;ETL\u0026rdquo; key Iterate over each key (e.g., \u0026ldquo;sales_data\u0026rdquo;) For each key, access its \u0026ldquo;metadata\u0026rdquo; to process the ETL steps. 3. ETL Steps Each ETL step (extract, transform, load) has: _before_sql: Queries to run first (setup). _sql: The main query or queries to run. _after_sql: Cleanup queries to run afterward. Queries can be: null: Do nothing. string: Reference a single query key in the same map or the query itself. array|list: Execute all queries in sequence. In case is not null it can be the query itself or just the name of a sql code block under the same key, where sql [query_name] or first line -- [query_name] Use _conn for connection settings. If null, fall back to the main connection. Additionally, error handling can be defined using [step]_on_err_match_patt and [step]_on_err_match_sql to handle specific database errors dynamically, where [step]_on_err_match_patt is the regexp patthern to match error,and if maches the [step]_on_err_match_sql is executed, the same can be applied for [step]_before_on_err_match_patt and [step]_before_on_err_match_sql. You can define patterns to match specific errors and provide SQL statements to resolve those errors. This feature is useful when working with dynamically created databases, tables, or schemas. 4. Output Logsn Log progress (e.g., connection usage, start/end times, descriptions). Gracefully handle missing or null keys. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/etl/","title":"ETL"},{"content":"Validation Rules Validate data quality during the ETL process by using the key [step]_validation in the metadata section. Example: 1 2 3 4 5 6 7 8 9 10 ... load_validation: - type: throw_if_empty sql: validate_data_not_empty msg: \u0026#39;No data extracted for the given date!\u0026#39; active: false - type: throw_if_not_empty sql: validate_data_duplicates msg: \u0026#39;Duplicate data detected!\u0026#39; ... For every object in the [step]_validation the sql is executed in the [step]_con connection, and it can either throw error message defined in msg or not if the condition (type:throw_if_empty | type:throw_if_not_empty or) is met or not.\nExtracting Data from Unsupported Databases If the database you are using does not have a direct DuckDB scanner, but it is supported by sqlx or it has odbc support, you must set the to_csv option to true in the extract configuration. This ensures that data is first exported to a CSV file and then on step load the file can be processed by DuckDB.\nExample Configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ... ## table_from_odbc_source ```yaml metadata name: table_from_odbc_source description: \u0026#39;This is an example o how to extract from databases that does not have a DuckDB scanner\u0026#39; to_csv: true extract_conn: \u0026#39;odbc:DRIVER={ODBC Driver 17 for SQL Server};SERVER=@MSSQL_HOST;UID=@MSSQL_USER;PWD=@MSSQL_PASS;DATABASE=DB\u0026#39; extract_sql: | SELECT [fields] FROM [table] WHERE [condition] load_conn: \u0026#39;duckdb:\u0026#39; load_before_sql: - load_extentions - conn load_sql: load_exported_csv load_after_sql: detaches ``` ... Once extracted, the CSV file can be loaded by DuckDB using: ```sql load_exported_csv CREATE OR REPLACE TABLE DB.target_table AS SELECT * FROM READ_CSV(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;, HEADER TRUE); ``` Why Use to_csv: true? Workaround for unsupported databases: If DuckDB does not have a direct scanner, exporting data to CSV allows and then the CSV can be loaded by the DuckDB. Ensures compatibility: ETLX will handle the conversion, making the data accessible for further transformation and loading. Required for smooth ETL workflows: Without this option, DuckDB may fail to recognize or query the database directly. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/validation/","title":"Validation Rules"},{"content":"Query Documentation In some ETL processes, particularly during the Transform step, queries may become too complex to manage as a single string. To address this, the configuration supports a structured approach where you can break down a query into individual fields and their respective SQL components. This approach improves modularity, readability, and maintainability.\nStructure A complex query is defined as a top-level heading (e.g., # My Complex Query) in the configuration. Each field included in the query is represented as a Level 2 heading (e.g., ## Field Name).\nFor each field:\nMetadata can describe the field (e.g., name, description) if a yaml metadata is not provided the field key is used as field in this example Field Name. SQL components like select, from, join, where, group_by, order_by, having, and cte are specified in separate sql blocks. Markdown Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # My Complex Query This query processes sales and regions data. ```yaml metadata name: sales_and_regions_query description: \u0026#34;Combines sales data with region metadata.\u0026#34; ``` ## Sales Field ```yaml metadata name: sales_field description: \u0026#34;Field representing sales data.\u0026#34; ``` ```sql -- select SELECT S.total_sales AS sales_field ``` ```sql -- from FROM sales_data AS S ``` ## Regions Field ```yaml metadata name: regions_field description: \u0026#34;Field representing region metadata.\u0026#34; ``` ```sql -- cte WITH region_cte AS ( SELECT region_id, region_name FROM region_data WHERE active = TRUE ) ``` ```sql -- select , R.region_name AS regions_field ``` ```sql -- join LEFT JOIN region_cte AS R ON S.region_id = R.region_id ``` ```sql -- where WHERE S.total_sales \u0026gt; 1000 ``` How Query Doc. Works Parsing the Configuration:\nEach query is parsed as a separate section with its metadata stored under the metadata key. Fields within the query are parsed as child sections, each containing its own metadata and SQL components. Combining the Query:\nThe query is built by iterating over the fields (in the order they appear) and combining their SQL components in the following order: cte ‚Üí select ‚Üí from ‚Üí join ‚Üí where ‚Üí group_by ‚Üí having ‚Üí order_by All the resulting parts are concatenated to form the final query. Why This Approach Matters Handling complex queries with hundreds of columns and numerous joins can quickly become overwhelming. By breaking down the query into smaller, manageable sections, you gain the ability to focus on individual components independently.\nThis approach is especially beneficial in a notebook-like environment, where each section is represented as a separate heading and can be linked via a table of contents. With this structure, you can:\nEnhance Documentation: Add context to each field, join condition, or transformation directly in the query configuration. Incorporate Formulas: Include relevant calculations or business logic alongside the query components. Promote Collaboration: Enable multiple contributors to work on different parts of the query without conflicts. This method simplifies the process of building and maintaining large queries while promoting organization, clarity, and reusability in your ETL workflows.\nExample Use Case for this type o query documentation Consider a scenario where you need to create a large report combining data from multiple sources. Instead of writing a single, monolithic SQL query, you can use this modular approach to:\nDefine each column or join independently. Add detailed documentation for each transformation step. Generate a table of contents for easier navigation and review. The result is a well-organized, maintainable, and self-documented query configuration that streamlines the development and review process.\nResulting Query For the example above, the generated query will look like this:\n1 2 3 4 5 6 7 8 9 10 WITH region_cte AS ( SELECT region_id, region_name FROM region_data WHERE active = TRUE ) SELECT S.total_sales AS sales_field , R.region_name AS regions_field FROM sales_data AS S LEFT JOIN region_cte AS R ON S.region_id = R.region_id WHERE S.total_sales \u0026gt; 1000 Metadata Options Query Metadata:\nname: A unique identifier for the query. description: A brief description of the query\u0026rsquo;s purpose. Field Metadata:\nname: A unique identifier for the field. description: A brief description of the field\u0026rsquo;s purpose. But if you only using the parser for you to document your queries you may want to pass extra information in your metadata to use to generate documentation like data lineage / dictionary, \u0026hellip; Benefits Modularity: Each field is defined separately, making the query easier to understand and modify. Reusability: SQL components like cte or join can be reused across different queries. Readability: Breaking down complex queries improves comprehension and reduces debugging time. By leveraging this structure, you can handle even the most complex SQL queries in your ETL process with ease and flexibility. Each query becomes manageable, and you gain the ability to compose intricate SQL logic dynamically.\n","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/query-doc/","title":"Query Documentation"},{"content":"Data Quality The DATA_QUALITY section allows you to define and execute validation rules to ensure the quality of your data. Each rule performs a check using a SQL query to identify records that violate a specific condition. Optionally, you can define a query to fix any identified issues automatically if applicable.\nData Quality Structure Metadata:\nThe DATA_QUALITY section contains metadata describing its purpose and activation status. Validation Rules:\nEach validation rule is defined as a Level 2 heading under the DATA_QUALITY block. Rules include a query to check for violations and, optionally, a query to fix issues. Execution:\nThe system loops through all rules in the DATA_QUALITY block. For each rule: Runs the validation query. If violations are found and a fix query is defined, executes the fix query. Data Quality Markdown Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 # DATA_QUALITY ```yaml description: \u0026#34;Runs some queries to check quality / validate.\u0026#34; active: true ``` ## Rule0001 ```yaml name: Rule0001 description: \u0026#34;Check if the field x has the option y and z.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; query: quality_check_query fix_quality_err: fix_quality_err_query column: total_reg_with_err # Defaults to \u0026#39;total\u0026#39;. check_only: false # runs only quality check if true fix_only: false # runs only quality fix if true and available and possible after_sql: \u0026#34;DETACH DB\u0026#34; active: true ``` ```sql -- quality_check_query SELECT COUNT(*) AS \u0026#34;total_reg_with_err\u0026#34; FROM \u0026#34;sales\u0026#34; WHERE \u0026#34;option\u0026#34; NOT IN (\u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;); ``` ```sql -- fix_quality_err_query UPDATE \u0026#34;sales\u0026#34; SET \u0026#34;option\u0026#34; = \u0026#39;default value\u0026#39; WHERE \u0026#34;option\u0026#34; NOT IN (\u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;); ``` ## Rule0002 ```yaml name: Rule0002 description: \u0026#34;Check if the field y has the option x and z.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; query: quality_check_query fix_quality_err: null # no automated fixing for this column: total_reg_with_err # Defaults to \u0026#39;total\u0026#39;. after_sql: \u0026#34;DETACH DB\u0026#34; active: true ``` ```sql -- quality_check_query SELECT COUNT(*) AS \u0026#34;total_reg_with_err\u0026#34; FROM \u0026#34;sales\u0026#34; WHERE \u0026#34;option2\u0026#34; NOT IN (\u0026#39;x\u0026#39;, \u0026#39;z\u0026#39;); ``` --- #### **How Data Quality Works** 1. **Defining Rules**: - Each rule specifies: - A SQL query (`query`) to validate data. - An optional fix query (`fix_quality_err`) to resolve issues. - Metadata for connection, pre/post-SQL commands, and status. 2. **Execution Flow**: - The validation query is executed first. - If the number of violations is greater than 0: - Logs the count of invalid records. - Executes the fix query if `fix_quality_err` is defined. 3. **Output**: - Provides detailed logs about rule violations and fixes applied. --- #### **Data Quality Example Use Case** For the example above: 1. **Rule0001**: - Validates that the `option` field contains only the values `y` and `z`. - Updates invalid records to a default value using the fix query. 2. **Rule0002**: - Validates that the `option2` field contains only the values `x` and `z`. - Updates invalid records to a default value using the fix query. --- #### **Data Quality Benefits** - **Automated Quality Assurance**: - Identify and fix data issues programmatically. - **Customizable Rules**: - Define rules tailored to your specific data quality requirements. - **Flexibility**: - Supports pre- and post-SQL commands for advanced workflows. By integrating the `DATA_QUALITY` block, you can ensure the integrity of your data and automate validation processes as part of your ETL pipeline. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/data-quality/","title":"Data Quality"},{"content":"Exports The EXPORTS section in the ETL configuration handles exporting data to files. This is particularly useful for generating reports for internal departments, regulators, partners, or saving processed data to a data lake. By leveraging DuckDB\u0026rsquo;s ability to export data in various formats, this section supports file generation with flexibility and precision.\nSales Data Export 1 2 3 4 5 6 name: DailyExports description: \u0026#34;Daily file exports for various datasets.\u0026#34; database: reporting_db connection: \u0026#34;duckdb:\u0026#34; path: \u0026#34;/path/to/Reports/YYYYMMDD\u0026#34; active: true 1 2 3 4 5 6 -- export COPY ( SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;Sales\u0026#34; WHERE \u0026#34;sale_date\u0026#34; = \u0026#39;{YYYY-MM-DD}\u0026#39; ) TO \u0026#39;/path/to/Reports/YYYYMMDD/sales_YYYYMMDD.csv\u0026#39; (FORMAT \u0026#39;csv\u0026#39;, HEADER true); Region Data Export to Excel 1 2 3 4 5 6 7 8 9 10 name: RegionExport description: \u0026#34;Export region data to an Excel file.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; export_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;LOAD excel\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; - export - \u0026#34;DETACH DB\u0026#34; active: true 1 2 3 4 5 6 -- export COPY ( SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;Regions\u0026#34; WHERE \u0026#34;updated_at\u0026#34; \u0026gt;= \u0026#39;{YYYY-MM-DD}\u0026#39; ) TO \u0026#39;/path/to/Reports/YYYYMMDD/regions_YYYYMMDD.xlsx\u0026#39; (FORMAT XLSX, HEADER TRUE); Sales Report Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 name: SalesReport description: \u0026#34;Generate a sales report from a template.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; template: \u0026#34;/path/to/Templates/sales_template.xlsx\u0026#34; path: \u0026#34;/path/to/Reports/sales_report_YYYYMMDD.xlsx\u0026#34; mapping: - sheet: Summary range: B2 sql: summary_query type: range table: SummaryTable table_style: TableStyleLight1 header: true if_exists: delete - sheet: Details range: A1 sql: details_query type: value key: total_sales after_sql: \u0026#34;DETACH DB\u0026#34; active: true 1 2 3 4 -- summary_query SELECT SUM(total_sales) AS total_sales FROM \u0026#34;DB\u0026#34;.\u0026#34;Sales\u0026#34; WHERE \u0026#34;sale_date\u0026#34; = \u0026#39;{YYYY-MM-DD}\u0026#39; 1 2 3 4 -- details_query SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;Sales\u0026#34; WHERE \u0026#34;sale_date\u0026#34; = \u0026#39;{YYYY-MM-DD}\u0026#39;; How Exporting It Works Parsing the Configuration:\nEach export is parsed as a separate section with its metadata stored under the metadata key. SQL queries or template mappings are defined as child sections. File Generation:\nThe export_sql field specifies a sequence of SQL statements used for the export. The final COPY statement defines the file format and location. Template-Based Exports:\nTemplates map query results to specific sheets and cells in an existing spreadsheet. The mapping field defines how query results populate the template: sheet: The target sheet in the template. range: The starting cell for the data. sql: The query generating the data. type: Indicates whether the data fills a range (range) or single value (value). table_style: The table style applied to the range. if_exists: Specifies how to handle existing data (e.g., delete or append). header: Whether to include headers in the exported table. clear_range: If true, clears the specified range before populating new data. clear_sheet: If true, clears all content from the specified sheet before populating new data. active: If false, skips this mapping. the maping can also be a string representing a query and all the mapping can be loaded from a table in the database to simplify the config, and also in a real world it can be extensive, would be easier to be done in a spreadsheet and loaded as a table. Resulting Outputs CSV File:\nExports sales data to a CSV file located at /path/to/Reports/YYYYMMDD/sales_YYYYMMDD.csv. Excel File:\nExports region data to an Excel file located at /path/to/Reports/YYYYMMDD/regions_YYYYMMDD.xlsx. Populated Template:\nGenerates a sales report from sales_template.xlsx and saves it as sales_report_YYYYMMDD.xlsx. Benefits of this functionality Flexibility: Export data in multiple formats (e.g., CSV, Excel) using DuckDB\u0026rsquo;s powerful COPY command. Reusability: Use predefined templates to create consistent reports. Customizability: SQL queries and mappings allow fine-grained control over the exported data. By leveraging the EXPORTS section, you can automate data export processes, making them efficient and repeatable.\nüìù Exporting as Text-Based Template In addition to exporting structured data formats like CSV or Excel, ETLX also supports exporting reports using plain-text templates such as HTML, XML, Markdown, etc.\nThese templates are rendered using Go‚Äôs text/template engine and can use dynamic data from SQL queries declared under the data_sql field, just like in the NOTIFY section.\nThis is especially useful for reporting, integration, or publishing documents with dynamic content.\nüì¶ Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ... ## TEXT_TMPL ```yaml metadata name: TEXT_TMPL description: \u0026#34;Export data to text base template\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;INSTALL sqlite\u0026#34; - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;database/HTTP_EXTRACT.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; data_sql: - logs - data after_sql: \u0026#34;DETACH DB\u0026#34; tmp_prefix: null text_template: true template: template return_content: false # if true, returns content instead of writing file path: \u0026#34;nyc_taxy_YYYYMMDD.html\u0026#34; active: true ``` ```sql -- data SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;NYC_TAXI\u0026#34; WHERE \u0026#34;tpep_pickup_datetime\u0026#34;::DATETIME \u0026lt;= \u0026#39;{YYYY-MM-DD}\u0026#39; LIMIT 100 ``` ```sql -- logs SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;etlx_logs\u0026#34; --WHERE \u0026#34;ref\u0026#34; = \u0026#39;{YYYY-MM-DD}\u0026#39; ``` ```html template \u0026lt;style\u0026gt; table { border-collapse: collapse; ... (truncated for space) ``` ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/exports/","title":"Exports"},{"content":"Scripts The SCRIPTS section allows you to execute SQL queries that don‚Äôt fit into other predefined sections (ETL, EXPORTS, etc.).\nüîπ When to Use SCRIPTS? ‚úÖ Running cleanup queries after an ETL job\n‚úÖ Executing ad-hoc maintenance tasks\n‚úÖ Running SQL commands that don‚Äôt need to return results\n‚úÖ Executing SQL scripts for database optimizations\nüõ† Example: Running Cleanup Scripts This example removes temporary data after an ETL process.\nüìÑ Markdown Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # SCRIPTS Run Queries that does not need a return ```yaml metadata name: DailyScripts description: \u0026#34;Daily Scripts\u0026#34; connection: \u0026#34;duckdb:\u0026#34; active: true ``` ## SCRIPT1 ```yaml metadata name: SCRIPT1 description: \u0026#34;Clean up auxiliar / temp data\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;INSTALL sqlite\u0026#34; - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;database/DB.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; script_sql: clean_aux_data on_err_patt: null on_err_sql: null after_sql: \u0026#34;DETACH DB\u0026#34; active: true ``` ```sql -- clean_aux_data DROP TEMP_TABLE1; ``` --- #### **üîπ How Scripts It Works** 1Ô∏è‚É£ **Loads necessary extensions and connects to the database.** 2Ô∏è‚É£ **Executes predefined SQL queries (`script_sql`).** 3Ô∏è‚É£ **Runs `before_sql` commands before execution.** 4Ô∏è‚É£ **Runs `after_sql` commands after execution.** ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/scripts/","title":"Scripts"},{"content":"Multi-Queries The MULTI_QUERIES section allows you to define multiple queries with similar structures and aggregate their results using a SQL UNION. This is particularly useful when generating summaries or reports that combine data from multiple queries into a single result set.\nMulti-Queries Structure Metadata:\nThe MULTI_QUERIES section includes metadata for connection details, pre/post-SQL commands, and activation status. The union_key defines how the queries are combined (e.g., UNION, UNION ALL). Query Definitions:\nEach query is defined as a Level 2 heading under the MULTI_QUERIES block. Queries inherit metadata from the parent block unless overridden. Execution:\nAll queries are combined using the specified union_key (default is UNION). The combined query is executed as a single statement. The combined query can be save by specifying save_sql normally an insert, create [or replace] table or even a copy to file statement, and insert statement should be used in combination with the save_on_err_patt and save_on_err_sql in case of an error matching the table does ... not exist to create the table instead. Multi-Queries Markdown Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # MULTI_QUERIES ```yaml description: \u0026#34;Define multiple structured queries combined with UNION.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; save_sql: save_mult_query_res save_on_err_patt: \u0026#39;(?i)table.+with.+name.+(\\w+).+does.+not.+exist\u0026#39; save_on_err_sql: create_mult_query_res after_sql: \u0026#34;DETACH DB\u0026#34; union_key: \u0026#34;UNION ALL\\n\u0026#34; # Defaults to UNION. active: true ``` ```sql -- save_mult_query_res INSERT INTO \u0026#34;DB\u0026#34;.\u0026#34;MULTI_QUERY\u0026#34; BY NAME [[final_query]] ``` ```sql -- create_mult_query_res CREATE OR REPLACE TABLE \u0026#34;DB\u0026#34;.\u0026#34;MULTI_QUERY\u0026#34; AS [[final_query]] ``` ## Row1 ```yaml name: Row1 description: \u0026#34;Row 1\u0026#34; query: row_query active: true ``` ```sql -- row_query SELECT \u0026#39;# number of rows\u0026#39; AS \u0026#34;variable\u0026#34;, COUNT(*) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; ``` ## Row2 ```yaml name: Row2 description: \u0026#34;Row 2\u0026#34; query: row_query active: true ``` ```sql -- row_query SELECT \u0026#39;total revenue\u0026#39; AS \u0026#34;variable\u0026#34;, SUM(\u0026#34;total\u0026#34;) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; ``` ## Row3 ```yaml name: Row3 description: \u0026#34;Row 3\u0026#34; query: row_query active: true ``` ```sql -- row_query SELECT \u0026#34;region\u0026#34; AS \u0026#34;variable\u0026#34;, SUM(\u0026#34;total\u0026#34;) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; GROUP BY \u0026#34;region\u0026#34; ``` Multi-Queries How It Works Defining Queries:\nQueries are defined as Level 2 headings. Each query can include its own metadata and SQL. Combining Queries:\nAll queries are combined using the union_key (e.g., UNION or UNION ALL). The combined query is executed as a single statement. Execution Flow:\nExecutes the before_sql commands at the start. Combines all active queries with the union_key. Executes the combined query. Executes the after_sql commands after the query execution. Multi-Queries Example Use Case For the example above:\nRow1: Counts the number of rows in the sales table. Row2: Calculates the total revenue from the sales table. Row3: Sums the revenue for each region in the sales table. The resulting combined query:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 LOAD sqlite; ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE); SELECT \u0026#39;# number of rows\u0026#39; AS \u0026#34;variable\u0026#34;, COUNT(*) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; UNION ALL SELECT \u0026#39;total revenue\u0026#39; AS \u0026#34;variable\u0026#34;, SUM(\u0026#34;total\u0026#34;) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; UNION ALL SELECT \u0026#34;region\u0026#34; AS \u0026#34;variable\u0026#34;, SUM(\u0026#34;total\u0026#34;) AS \u0026#34;value\u0026#34; FROM \u0026#34;sales\u0026#34; GROUP BY \u0026#34;region\u0026#34;; DETACH DB; Multi-Queries Benefits ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/multi-queries/","title":"Multi-Queries"},{"content":"Logs Handling (# LOGS) ETLX provides a logging mechanism that allows saving logs into a database. This is useful for tracking executions, debugging, and auditing ETL processes.\nüîπ How It Works The LOGS section defines where and how logs should be saved. The process consists of three main steps: Prepare the environment using before_sql (e.g., loading extensions, attaching databases). Execute save_log_sql to store logs in the database. Run after_sql for cleanup (e.g., detaching the database). üõ† Example LOGS Configuration Below is an example that saves logs into a database:\nüìÑ LOGS Markdown Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 # LOGS ```yaml metadata name: LOGS description: \u0026#34;Example saving logs\u0026#34; table: logs connection: \u0026#34;duckdb:\u0026#34; before_sql: - load_extentions - attach_db save_log_sql: load_logs save_on_err_patt: \u0026#39;(?i)table.+with.+name.+(\\w+).+does.+not.+exist\u0026#39; save_on_err_sql: create_logs after_sql: detach_db tmp_dir: tmp active: true ``` ```sql -- load_extentions INSTALL Sqlite; LOAD Sqlite; INSTALL json; LOAD json; ``` ```sql -- attach_db ATTACH \u0026#39;examples/S3_EXTRACT.db\u0026#39; AS \u0026#34;DB\u0026#34; (TYPE SQLITE) ``` ```sql -- detach_db DETACH \u0026#34;DB\u0026#34;; ``` ```sql -- load_logs INSERT INTO \u0026#34;DB\u0026#34;.\u0026#34;\u0026lt;table\u0026gt;\u0026#34; BY NAME SELECT * FROM READ_JSON(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;); ``` ```sql -- create_logs CREATE OR REPLACE TABLE \u0026#34;DB\u0026#34;.\u0026#34;\u0026lt;table\u0026gt;\u0026#34; AS SELECT * FROM READ_JSON(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;); ``` --- ### **üîπ How to Use** - This example saves logs into a **SQLite database attached to DuckDB**. - The **log table (`logs`) is created or replaced** on each run. - The `\u0026lt;table\u0026gt;` and `\u0026lt;fname\u0026gt;` placeholders are dynamically replaced. ### **üéØ Summary** ‚úî **Keeps a persistent log of ETL executions** ‚úî **Uses DuckDB for efficient log storage** ‚úî **Supports preprocessing (`before_sql`) and cleanup (`after_sql`)** ‚úî **Highly customizable to different logging needs** ### Default logs By default is generated a sqlite db `etlx_logs.db` in temp folder, that\u0026#39;ll depende on the OS, it adds to your config this peace os md: ````markdown # AUTO_LOGS ```yaml metadata name: LOGS description: \u0026#34;Logging\u0026#34; table: logs connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD Sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;\u0026lt;tmp\u0026gt;/etlx_logs.db\u0026#39; (TYPE SQLITE)\u0026#34; - \u0026#34;USE etlx_logs\u0026#34; - \u0026#34;LOAD json\u0026#34; - \u0026#34;get_dyn_queries[create_missing_columns](ATTACH \u0026#39;\u0026lt;tmp\u0026gt;/etlx_logs.db\u0026#39; (TYPE SQLITE),DETACH etlx_logs)\u0026#34; save_log_sql: | INSERT INTO \u0026#34;etlx_logs\u0026#34;.\u0026#34;\u0026lt;table\u0026gt;\u0026#34; BY NAME SELECT * FROM READ_JSON(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;); save_on_err_patt: \u0026#39;(?i)table.+with.+name.+(\\w+).+does.+not.+exist\u0026#39; save_on_err_sql: | CREATE TABLE \u0026#34;etlx_logs\u0026#34;.\u0026#34;\u0026lt;table\u0026gt;\u0026#34; AS SELECT * FROM READ_JSON(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;); after_sql: - \u0026#39;USE memory\u0026#39; - \u0026#39;DETACH \u0026#34;etlx_logs\u0026#34;\u0026#39; active: true ``` ```sql -- create_missing_columns WITH source_columns AS ( SELECT \u0026#34;column_name\u0026#34;, \u0026#34;column_type\u0026#34; FROM (DESCRIBE SELECT * FROM READ_JSON(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;)) ), destination_columns AS ( SELECT \u0026#34;column_name\u0026#34;, \u0026#34;data_type\u0026#34; as \u0026#34;column_type\u0026#34; FROM \u0026#34;duckdb_columns\u0026#34; WHERE \u0026#34;table_name\u0026#34; = \u0026#39;\u0026lt;table\u0026gt;\u0026#39; ), missing_columns AS ( SELECT \u0026#34;s\u0026#34;.\u0026#34;column_name\u0026#34;, \u0026#34;s\u0026#34;.\u0026#34;column_type\u0026#34; FROM source_columns \u0026#34;s\u0026#34; LEFT JOIN destination_columns \u0026#34;d\u0026#34; ON \u0026#34;s\u0026#34;.\u0026#34;column_name\u0026#34; = \u0026#34;d\u0026#34;.\u0026#34;column_name\u0026#34; WHERE \u0026#34;d\u0026#34;.\u0026#34;column_name\u0026#34; IS NULL ) SELECT \u0026#39;ALTER TABLE \u0026#34;etlx_logs\u0026#34;.\u0026#34;\u0026lt;table\u0026gt;\u0026#34; ADD COLUMN \u0026#34;\u0026#39; || \u0026#34;column_name\u0026#34; || \u0026#39;\u0026#34; \u0026#39; || \u0026#34;column_type\u0026#34; || \u0026#39;;\u0026#39; AS \u0026#34;query\u0026#34; FROM missing_columns WHERE (SELECT COUNT(*) FROM destination_columns) \u0026gt; 0; ``` --- But it can be overiden to be saved on your own database of choice by changing `ATTACH \u0026#39;\u0026lt;tmp\u0026gt;/etlx_logs.db\u0026#39; (TYPE SQLITE)` ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/logs/","title":"Logs"},{"content":"REQUIRES The REQUIRES section in the ETL configuration allows you to load dependencies from external Markdown configurations. These dependencies can either be loaded from file paths or dynamically through queries. This feature promotes modularity and reusability by enabling you to define reusable parts of the configuration in separate files or queries.\nLoading Structure Metadata:\nThe REQUIRES section includes metadata describing its purpose and activation status. Loading Options:\nFrom Queries: Dynamically fetch configuration content from a query. Specify the query, column containing the configuration, and optional pre/post SQL scripts. From Files: Load configuration content from an external file path. Integration:\nThe loaded configuration is merged into the main configuration. Top-level headings in the required configuration that don‚Äôt exist in the main configuration are added. Loading Markdown Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # REQUIRES ```yaml description: \u0026#34;Load configuration dependencies from files or queries.\u0026#34; active: true ``` ## Sales Transformation ```yaml name: SalesTransform description: \u0026#34;Load sales transformation config from a query.\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;reporting.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; query: get_sales_conf column: md_conf_content # Defaults to \u0026#39;conf\u0026#39; if not provided. after_sql: \u0026#34;DETACH DB\u0026#34; active: false ``` ```sql -- get_sales_conf SELECT \u0026#34;md_conf_content\u0026#34; FROM \u0026#34;configurations\u0026#34; WHERE \u0026#34;config_name\u0026#34; = \u0026#39;Sales\u0026#39; AND \u0026#34;active\u0026#34; = true AND \u0026#34;excluded\u0026#34; = false; ``` ## Inventory Transformation ```yaml name: InventoryTransform description: \u0026#34;Load inventory transformation config from a file.\u0026#34; path: \u0026#34;/path/to/Configurations/inventory_transform.md\u0026#34; active: true ``` --- #### **How Loading Works** 1. **Defining Dependencies**: - Dependencies are listed as child sections under the `# REQUIRES` heading. - Each dependency specifies its source (`query` or `path`) and associated metadata. 2. **From Queries**: - Use the `query` field to specify a SQL query that retrieves the configuration. - The `column` field specifies which column contains the Markdown configuration content. - Optionally, use `before_sql` and `after_sql` to define scripts to run before or after executing the query. 3. **From Files**: - Use the `path` field to specify the file path of an external Markdown configuration. 4. **Merging with Main Configuration**: - After loading the configuration, any top-level headings in the loaded configuration that don‚Äôt exist in the main configuration are added. --- #### **Loading - Example Use Case** For the example above, the following happens: 1. **Sales Transformation**: - A query retrieves the Markdown configuration content for sales transformations from a database table. - The `before_sql` and `after_sql` scripts prepare the environment for the query execution. 2. **Inventory Transformation**: - A Markdown configuration is loaded from an external file path (`/path/to/Configurations/inventory_transform.md`). --- #### **Loading - Benefits** - **Modularity**: - Break large configurations into smaller, reusable parts. - **Dynamic Updates**: - Use queries to dynamically load updated configurations from databases. - **Ease of Maintenance**: - Keep configurations for different processes in separate files or sources, simplifying updates and version control. By leveraging the `REQUIRES` section, you can maintain a clean and scalable ETL configuration structure, promoting reusability and modular design. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/requires/","title":"Loading Config Dependencies"},{"content":"ACTIONS There are scenarios in ETL workflows where actions such as downloading, uploading, compressing or copying files cannot be performed using SQL alone. The ACTIONS section allows you to define steps for copying or transferring files using the file system or external protocols.\nACTIONS Structure Each action under the ACTIONS section has the following:\nname: Unique name for the action. description: Human-readable explanation. type: The kind of action to perform. Options: copy_file compress decompress ftp_download ftp_upload sftp_download sftp_upload http_download http_upload s3_download s3_upload db_2_db params: A map of input parameters required by the action type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 # ACTIONS ```yaml metadata name: FileOperations description: \u0026#34;Transfer and organize generated reports\u0026#34; path: examples active: true ``` --- ## COPY LOCAL FILE ```yaml metadata name: CopyReportToArchive description: \u0026#34;Move final report to archive folder\u0026#34; type: copy_file params: source: \u0026#34;C:/reports/final_report.xlsx\u0026#34; target: \u0026#34;C:/reports/archive/final_report_YYYYMMDD.xlsx\u0026#34; active: true ``` --- ## Compress to ZIP ```yaml metadata name: CompressReports description: \u0026#34;Compress report files into a .zip archive\u0026#34; type: compress params: compression: zip files: - \u0026#34;reports/report_1.csv\u0026#34; - \u0026#34;reports/report_2.csv\u0026#34; output: \u0026#34;archives/reports_YYYYMM.zip\u0026#34; active: true ``` ## UNZIP ```yaml metadata name: CompressReports description: \u0026#34;Compress report files into a .zip archive\u0026#34; type: decompress params: compression: zip input: \u0026#34;archives/reports_YYYYMM.zip\u0026#34; output: \u0026#34;tmp\u0026#34; active: true ``` --- ## Compress to GZ ```yaml metadata name: CompressToGZ description: \u0026#34;Compress a summary file to .gz\u0026#34; type: compress params: compression: gz files: - \u0026#34;reports/summary.csv\u0026#34; output: \u0026#34;archives/summary_YYYYMM.csv.gz\u0026#34; active: true ``` --- ## HTTP DOWNLOAD ```yaml metadata name: DownloadFromAPI description: \u0026#34;Download dataset from HTTP endpoint\u0026#34; type: http_download params: url: \u0026#34;https://api.example.com/data\u0026#34; target: \u0026#34;data/today.json\u0026#34; method: GET headers: Authorization: \u0026#34;Bearer @API_TOKEN\u0026#34; Accept: \u0026#34;application/json\u0026#34; params: date: \u0026#34;YYYYMMDD\u0026#34; limit: \u0026#34;1000\u0026#34; active: true ``` --- ## HTTP UPLOAD ```yaml metadata name: PushReportToWebhook description: \u0026#34;Upload final report to an HTTP endpoint\u0026#34; type: http_upload params: url: \u0026#34;https://webhook.example.com/upload\u0026#34; method: POST source: \u0026#34;reports/final.csv\u0026#34; headers: Authorization: \u0026#34;Bearer @WEBHOOK_TOKEN\u0026#34; Content-Type: \u0026#34;multipart/form-data\u0026#34; params: type: \u0026#34;summary\u0026#34; date: \u0026#34;YYYYMMDD\u0026#34; active: true ``` --- ## FTP DOWNLOAD ```yaml metadata name: FetchRemoteReport description: \u0026#34;Download data file from external FTP\u0026#34; type: ftp_download params: host: \u0026#34;ftp.example.com\u0026#34; port: \u0026#34;21\u0026#34; user: \u0026#34;myuser\u0026#34; password: \u0026#34;@FTP_PASSWORD\u0026#34; source: \u0026#34;/data/daily_report.csv\u0026#34; target: \u0026#34;downloads/daily_report.csv\u0026#34; active: true ``` ## FTP DOWNLOAD GLOB ```yaml metadata name: FetchRemoteReport2024 description: \u0026#34;Download data file from external FTP\u0026#34; type: ftp_download params: host: \u0026#34;ftp.example.com\u0026#34; port: \u0026#34;21\u0026#34; user: \u0026#34;myuser\u0026#34; password: \u0026#34;@FTP_PASSWORD\u0026#34; source: \u0026#34;/data/daily_report_2024*.csv\u0026#34; target: \u0026#34;downloads/\u0026#34; active: true ``` ## SFTP DOWNLOAD ```yaml metadata name: FetchRemoteReport description: \u0026#34;Download data file from external SFTP\u0026#34; type: stp_download params: host: \u0026#34;sftp.example.com\u0026#34; user: \u0026#34;myuser\u0026#34; password: \u0026#34;@SFTP_PASSWORD\u0026#34; host_key: ~/.ssh/known_hosts # or a specific file port: 22 source: \u0026#34;/data/daily_report.csv\u0026#34; target: \u0026#34;downloads/daily_report.csv\u0026#34; active: true ``` --- ## S3 UPLOAD ```yaml metadata name: ArchiveToS3 description: \u0026#34;Send latest results to S3 bucket\u0026#34; type: s3_upload params: AWS_ACCESS_KEY_ID: \u0026#39;@AWS_ACCESS_KEY_ID\u0026#39; AWS_SECRET_ACCESS_KEY: \u0026#39;@AWS_SECRET_ACCESS_KEY\u0026#39; AWS_REGION: \u0026#39;@AWS_REGION\u0026#39; AWS_ENDPOINT: 127.0.0.1:3000 S3_FORCE_PATH_STYLE: true S3_DISABLE_SSL: false S3_SKIP_SSL_VERIFY: true bucket: \u0026#34;my-etlx-bucket\u0026#34; key: \u0026#34;exports/summary_YYYYMMDD.xlsx\u0026#34; source: \u0026#34;reports/summary.xlsx\u0026#34; active: true ``` ## S3 DOWNLOAD ```yaml metadata name: DownalodFromS3 description: \u0026#34;Download file S3 from bucket\u0026#34; type: s3_download params: AWS_ACCESS_KEY_ID: \u0026#39;@AWS_ACCESS_KEY_ID\u0026#39; AWS_SECRET_ACCESS_KEY: \u0026#39;@AWS_SECRET_ACCESS_KEY\u0026#39; AWS_REGION: \u0026#39;@AWS_REGION\u0026#39; AWS_ENDPOINT: 127.0.0.1:3000 S3_FORCE_PATH_STYLE: true S3_DISABLE_SSL: false S3_SKIP_SSL_VERIFY: true bucket: \u0026#34;my-etlx-bucket\u0026#34; key: \u0026#34;exports/summary_YYYYMMDD.xlsx\u0026#34; target: \u0026#34;reports/summary.xlsx\u0026#34; active: true ``` --- ##### üì• ACTIONS ‚Äì `db_2_db` (Cross-Database Write) \u0026gt; As of this moment, **DuckDB does not support direct integration** with certain databases like **MSSQL**, **DB2**, or **Oracle**, the same way it does with **SQLite**, **Postgres**, or **MySQL**. To bridge this gap, the `db_2_db` action type allows you to **query data from one database** (source) and **write the results into another** (target), using ETLX‚Äôs internal execution engine (powered by `sqlx` or ODBC). ###### ‚úÖ Use Case Use `db_2_db` when: - Your database is not accessible with DuckDB. - You want to move data from one place to another using **pure SQL**, chunked if necessary. --- ###### üß© Example ````markdown ... ## WRITE_RESULTS_MSSQL ```yaml metadata name: WRITE_RESULTS_MSSQL description: \u0026#34;MSSQL example ‚Äì moving logs into a SQL Server database.\u0026#34; type: db_2_db params: source: conn: sqlite3:database/HTTP_EXTRACT.db before: null chunk_size: 1000 timeout: 30 sql: origin_query after: null target: conn: mssql:sqlserver://sa:@MSSQL_PASSWORD@localhost?database=master\u0026amp;connection+timeout=30 timeout: 30 before: - create_schema sql: mssql_sql after: null active: true ``` ```sql -- origin_query SELECT \u0026#34;description\u0026#34;, \u0026#34;duration\u0026#34;, STRFTIME(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, \u0026#34;start_at\u0026#34;) AS \u0026#34;start_at\u0026#34;, \u0026#34;ref\u0026#34; FROM \u0026#34;etlx_logs\u0026#34; ORDER BY \u0026#34;start_at\u0026#34; DESC ``` ```sql -- create_schema IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = \u0026#39;etlx_logs\u0026#39; AND type = \u0026#39;U\u0026#39;) CREATE TABLE [dbo].[etlx_logs] ( [description] NVARCHAR(MAX) NULL, [duration] BIGINT NULL, [start_at] DATETIME NULL, [ref] DATE NULL ); ``` ```sql -- mssql_sql INSERT INTO [dbo].[etlx_logs] ([:columns]) VALUES ``` ``` --- ### üõ†Ô∏è Notes ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/actions/","title":"Actions"},{"content":"NOTIFY The NOTIFY section enables sending notifications (e.g., email via SMTP) with dynamic templates populated from SQL query results. This is useful for monitoring ETL processes and sending status reports.\nWhy Use NOTIFY? ‚úÖ Real-time updates on ETL status\n‚úÖ Customizable email templates with dynamic content\n‚úÖ Supports attachments for automated reporting\n‚úÖ Ensures visibility into ETL success or failure\nExample: Sending ETL Status via Email This example sends an email after an ETL process completes, using log data from the database.\nNOTIFY Markdown Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 # NOTIFY ```yaml metadata name: Notification description: \u0026#34;ETL Notification\u0026#34; connection: \u0026#34;duckdb:\u0026#34; path: \u0026#34;examples\u0026#34; active: true ``` ## ETL_STATUS ```yaml metadata name: ETL_STATUS description: \u0026#34;ETL Status\u0026#34; connection: \u0026#34;duckdb:\u0026#34; before_sql: - \u0026#34;INSTALL sqlite\u0026#34; - \u0026#34;LOAD sqlite\u0026#34; - \u0026#34;ATTACH \u0026#39;database/HTTP_EXTRACT.db\u0026#39; AS DB (TYPE SQLITE)\u0026#34; data_sql: - logs after_sql: \u0026#34;DETACH DB\u0026#34; to: - real.datadriven@gmail.com cc: null bcc: null subject: \u0026#39;ETLX YYYYMMDD\u0026#39; body: body_tml attachments: - hf.md - http.md active: true ``` The **email body** is defined using a **Golang template**. The results from `data_sql` are available inside the template, also the Spring (`github.com/Masterminds/sprig`) library that provides more than 100 commonly used template functions. ```html body_tml \u0026lt;style\u0026gt; table { border-collapse: collapse; width: 100%; font-family: sans-serif; font-size: 14px; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f2f2f2; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } tr:hover { box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2); background-color: #eef6ff; } \u0026lt;/style\u0026gt; \u0026lt;b\u0026gt;Good Morning!\u0026lt;/b\u0026gt;\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt; This email was gebnerated by ETLX automatically!\u0026lt;br /\u0026gt; LOGS:\u0026lt;br /\u0026gt; {{ with .logs }} {{ if eq .success true }} \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Ref\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Start\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;End\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Duration\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Success\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Message\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; {{ range .data }} \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;{{ .name }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .ref }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .start_at | date \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .end_at | date \u0026#34;2006-01-02 15:04:05\u0026#34; }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ divf .duration 1000000000 | printf \u0026#34;%.4fs\u0026#34; }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .success }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;span title=\u0026#34;{{ .msg }}\u0026#34;\u0026gt;{{ .msg | toString | abbrev 30}}\u0026lt;/span\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {{ else }} \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026#34;7\u0026#34;\u0026gt;No items available\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {{ end }} \u0026lt;/table\u0026gt; {{ else }} \u0026lt;p\u0026gt;{{.msg}}\u0026lt;/p\u0026gt; {{ end }} {{ else }} \u0026lt;p\u0026gt;Logs information missing.\u0026lt;/p\u0026gt; {{ end }} ``` ```sql -- logs SELECT * FROM \u0026#34;DB\u0026#34;.\u0026#34;etlx_logs\u0026#34; WHERE \u0026#34;ref\u0026#34; = \u0026#39;{YYYY-MM-DD}\u0026#39; ``` --- #### **How NOTIFY Works** 1Ô∏è‚É£ **Loads required extensions and connects to the database** (`before_sql`). 2Ô∏è‚É£ **Executes `data_sql` queries** to retrieve data to be embeded in the body of the email. 3Ô∏è‚É£ **Uses the results inside the `body` template** (Golang templating). 4Ô∏è‚É£ **Sends an email with the formatted content and attachments.** 5Ô∏è‚É£ **Executes cleanup queries (`after_sql`).** #### **Key NOTIFY Features** ‚úî **Dynamic email content populated from SQL queries** ‚úî **Supports `to`, `cc`, `bcc`, `attachments`, and templated bodies** ‚úî **Executes SQL before and after sending notifications** ‚úî **Ensures ETL monitoring and alerting** ","description":null,"permalink":"https://realdatadriven.github.io/etlx/configuration/notify/","title":"Notify"},{"content":"Dynamic Query Generation (get_dyn_queries[...]) In some advanced ETL workflows, you may need to dynamically generate SQL queries based on metadata or schema differences between the source and destination databases.\nüîπ Why Use Dynamic Queries? ‚úÖ Schema Flexibility ‚Äì Automatically adapt to schema changes in the source system.\n‚úÖ Self-Evolving Workflows ‚Äì ETL jobs can generate and execute additional SQL queries as needed.\n‚úÖ Automation ‚Äì Reduces the need for manual intervention when new columns appear.\nüîπ How get_dyn_queries[query_name](runs_before,runs_after) Works Dynamic queries are executed using the get_dyn_queries[query_name](runs_before,runs_after) pattern. During execution, ETLX runs the query query_name and retrieves dynamically generated queries. The resulting queries are then executed automatically. üõ† Example: Auto-Adding Missing Columns This example checks for new columns in a JSON file and adds them to the destination table.\nüìÑ Markdown Configuration for get_dyn_queries[query_name](runs_before,runs_after) If the query_name depends on attaching and detaching the main db where it will run, those should be passed as dependencies, because the dynamic queries are generate before any other query and put in the list for the list where it is to be executed, to be a simpler flow, but they are optional otherwise.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 .... ```yaml metadata ... connection: \u0026#34;duckdb:\u0026#34; before_sql: - ... - get_dyn_queries[create_missing_columns] # Generates queries defined in `create_missing_columns` and Executes them .. ``` **üìú SQL Query (Generating Missing Columns)** ```sql -- create_missing_columns WITH source_columns AS ( SELECT column_name, column_type FROM (DESCRIBE SELECT * FROM read_json(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;)) ), destination_columns AS ( SELECT column_name, data_type as column_type FROM duckdb_columns WHERE table_name = \u0026#39;\u0026lt;table\u0026gt;\u0026#39; ), missing_columns AS ( SELECT s.column_name, s.column_type FROM source_columns s LEFT JOIN destination_columns d ON s.column_name = d.column_name WHERE d.column_name IS NULL ) SELECT \u0026#39;ALTER TABLE \u0026#34;\u0026lt;table\u0026gt;\u0026#34; ADD COLUMN \u0026#34;\u0026#39; || column_name || \u0026#39;\u0026#34; \u0026#39; || column_type || \u0026#39;;\u0026#39; AS query FROM missing_columns WHERE (SELECT COUNT(*) FROM destination_columns) \u0026gt; 0; ``` --- #### **üõ† Execution Flow** 1Ô∏è‚É£ **Extract column metadata from the input (in this case a json file, but it could be a table or any other valid query).** 2Ô∏è‚É£ **Check which columns are missing in the destination table (`\u0026lt;table\u0026gt;`).** 3Ô∏è‚É£ **Generate `ALTER TABLE` statements for adding missing columns, and replaces the `- get_dyn_queries[create_missing_columns]` with the the generated queries** 4Ô∏è‚É£ **Runs the workflow with dynamically generated queries against the destination connection.** #### **üîπ Key Features** ‚úî **Fully automated schema updates** ‚úî **Works with flexible schema data (e.g., JSON, CSV, Parquet, etc.)** ‚úî **Reduces manual maintenance when source schemas evolve** ‚úî **Ensures destination tables always match source structure** --- **With `get_dyn_queries[...]`, your ETLX workflows can now dynamically evolve with changing data structures!** ","description":null,"permalink":"https://realdatadriven.github.io/etlx/advanced/dynamic-queries/","title":"Dynamic Queries"},{"content":"Conditional Execution ETLX allows conditional execution of SQL blocks based on the results of a query. This is useful to skip operations dynamically depending on data context (e.g., skip a step if no new data is available, or if a condition in the target is not met.\nYou can define condition blocks using the following keys:\nFor ETL step-specific conditions:\nextract_condition transform_condition load_condition etc. For generic sections (e.g., DATA_QUALITY, EXPORTS, NOTIFY, etc.):\ncondition You can also specify an optional *condition_msg to log a custom message when a condition is not met.\nCondition Evaluation Logic The SQL query defined in *_condition or condition is executed. The result must mast be boolean. If not met, the corresponding main logig will be skipped. If *_condition_msg is provided, it will be included in the log entry instead of the default skip message. Example ‚Äì Conditional Load Step 1 2 3 4 load_conn: \u0026#34;duckdb:\u0026#34; load_condition: check_load_required load_condition_msg: \u0026#34;No new records to load today\u0026#34; load_sql: perform_load 1 2 -- check_load_required SELECT COUNT(*) \u0026gt; 0 as _check FROM staging_table WHERE processed = false; 1 2 3 -- perform_load INSERT INTO target_table SELECT * FROM staging_table WHERE processed = false; Example ‚Äì Global Conditional Notification 1 2 3 4 5 6 type: notify name: notify_if_failures description: \u0026#34;Send email only if failures occurred\u0026#34; connection: \u0026#34;duckdb:\u0026#34; condition: check_failures condition_msg: \u0026#34;No failures detected, no email sent\u0026#34; 1 2 -- check_failures SELECT COUNT(*) \u0026gt; 0 as chk FROM logs WHERE success = false; üìù Note: If no *_condition_msg is defined and the condition fails, ETLX will simply log the skipped step with a standard message like:\n\u0026quot;Condition 'load_condition' was not met. Skipping step 'load'.\u0026quot;\n","description":null,"permalink":"https://realdatadriven.github.io/etlx/advanced/conditional-execution/","title":"Conditional Execution"},{"content":"Advanced Workflow Execution: runs_as Override The runs_as field in the metadata block of any Level 1 key allows ETLX to treat a custom section as a built-in block (like ETL, EXPORTS, etc.), enabling advanced chaining of processes within the same configuration.\n1 2 3 4 5 6 7 8 9 10 11 12 # ETL_AFTER_SOME_KEY ```yaml metadata runs_as: ETL description: \u0026#34;Post-validation data transformation\u0026#34; active: true ``` ## ETL_OVER_SOME_MAIN_STEP ... In this example:\nETLX will run the original ETL block. Then execute DATA_QUALITY, an so on. Then treat ETL_AFTER_SOME_KEY as another ETL block (since it contains runs_as: ETL) and execute it as such. This allows chaining of processes within the same configuration file.\n‚ö†Ô∏è Order Matters The custom section (e.g. # ETL_AFTER_SOME_KEY) is executed in the order it appears in the Markdown file after the main keys. That means the flow becomes:\n# ETL # DATA_QUALITY # ETL2 (runs as ETL) This enables advanced chaining like:\nExporting logs after validation. Reapplying transformations based on data quality feedback. Generating post-validation reports. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/advanced/runs-as/","title":"runs_as override"},{"content":"Embedding in Go To embed the ETL framework in a Go application, you can use the etlx package and call ConfigFromMDText and RunETL. Example (from README):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/realdatadriven/etlx\u0026#34; ) func main() { etl := \u0026amp;etlx.ETLX{} // Load configuration from Markdown text err := etl.ConfigFromMDText(`# Your Markdown config here`) if err != nil { fmt.Printf(\u0026#34;Error loading config: %v\\n\u0026#34;, err) return } // Prepare date reference dateRef := []time.Time{time.Now().AddDate(0, 0, -1)} // Define additional options options := map[string]any{ \u0026#34;only\u0026#34;: []string{\u0026#34;sales\u0026#34;}, \u0026#34;steps\u0026#34;: []string{\u0026#34;extract\u0026#34;, \u0026#34;load\u0026#34;}, } // Run ETL process logs, err := etl.RunETL(dateRef, nil, options) if err != nil { fmt.Printf(\u0026#34;Error running ETL: %v\\n\u0026#34;, err) return } // Print logs for _, log := range logs { fmt.Printf(\u0026#34;Log: %+v\\n\u0026#34;, log) } } ","description":null,"permalink":"https://realdatadriven.github.io/etlx/embedding/","title":"Embedding in Go"},{"content":"Future Plans ETLX is a powerful tool for defining and executing ETL processes using Markdown configuration files. It supports complex SQL queries, exports to multiple formats, and dynamic configuration loading. ETLX can be used as a library, CLI tool, or integrated into other systems for advanced data workflows.\nTo-Do List ‚úÖ Completed Config Parsing:\nParses and validates Markdown configurations with nested sections and metadata. Supports YAML, TOML, and JSON for metadata. ETL Execution:\nModular handling of extract, transform, and load processes. Flexible step configuration with before and after SQL. Query Documentation:\nHandles complex SQL queries by breaking them into logical components. Dynamically combines query parts to create executable SQL. Exports:\nSupports exporting data to files in formats like CSV and Excel. Includes options for templates and data mapping. Requires:\nLoads additional configurations dynamically from files or database queries. Integrates loaded configurations into the main process. CLI Interface:\nProvides a command-line interface for running configurations. Accepts flags for custom execution parameters. üïí To-Do Web API:\nCreate a RESTful web API for executing ETL configurations. Expose endpoints for: Uploading and managing configurations. Triggering ETL workflows. Monitoring job status and logs. Add support for multi-user environments with authentication and authorization. ","description":null,"permalink":"https://realdatadriven.github.io/etlx/future/","title":"Future Plans"},{"content":"License This project is licensed under the MIT License.\n","description":null,"permalink":"https://realdatadriven.github.io/etlx/license/","title":"License"},{"content":"Installation (new site) Create a new Hugo site with Plume:\nhugo new site mysite cd mysite hugo mod init github.com/\u0026lt;username\u0026gt;/\u0026lt;site\u0026gt; hugo mod tidy hugo mod get github.com/Dobefu/hugo-theme-plume Add to your hugo.toml:\n1 2 3 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; Installation (existing site) As a Hugo Module hugo mod init github.com/\u0026lt;username\u0026gt;/\u0026lt;site\u0026gt; hugo mod tidy hugo mod get github.com/Dobefu/hugo-theme-plume Add to your hugo.toml:\n1 2 3 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; As a Git Submodule git submodule add https://github.com/Dobefu/hugo-theme-plume.git themes/plume Add to your hugo.toml:\n1 theme = \u0026#34;plume\u0026#34; Configuration To configure the theme, update the hugo.toml file with the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; [markup] [markup.tableOfContents] startLevel = 2 endLevel = 4 [markup.highlight] lineNos = true noClasses = false [markup.goldmark] [markup.goldmark.extensions] strikethrough = false [markup.goldmark.extensions.extras] delete.enable = true mark.enable = true insert.enable = true subscript.enable = true superscript.enable = true [markup.goldmark.extensions.passthrough] enable = true [markup.goldmark.extensions.passthrough.delimiters] block = [[\u0026#39;\\[\u0026#39;, \u0026#39;\\]\u0026#39;]] inline = [[\u0026#39;\\(\u0026#39;, \u0026#39;\\)\u0026#39;]] ","description":"Installation instructions for the Plume theme.","permalink":"https://realdatadriven.github.io/etlx/docs/getting-started/installation/","title":"Installation"}]