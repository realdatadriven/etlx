[{"content":"1. Introduction ETLX introduces an innovative, flexible way to handle ETL processes, data quality validation, and report automation. It empowers users with:\nMarkdown-driven Configuration: Define your workflows in a structured, human-readable format. DuckDB Integration: Utilize DuckDB‚Äôs capabilities to work with diverse data sources, including databases, object storage, and APIs. Extensibility: Easily adapt to new use cases with modular configurations. ","description":null,"permalink":"https://example.org/introduction/","title":"Introduction"},{"content":"Installation Option 1: Precompiled Binaries Precompiled binaries for Linux, macOS, and Windows are available on the releases page. Download the appropriate binary for your system and make it executable:\n1 2 3 # Example for Linux or macOS chmod +x etlx ./etlx --help Option 2: Install via Go (as a library) 1 2 # Install ETLX go get github.com/realdatadriven/etlx Option 3: Clone Repo 1 2 git clone https://github.com/realdatadriven/etlx.git cd etlx And then:\n1 go run cmd/main.go --config etl_config.md --date 2023-10-31 On Windows you may have build issues; in that case using the latest libduckdb from duckdb/releases and building with -tags=duckdb_use_lib may help:\n1 CGO_ENABLED=1 CGO_LDFLAGS=\u0026#34;-L/path/to/libs\u0026#34; go run -tags=duckdb_use_lib main.go --config etl_config.md --date 2023-10-31 ","description":null,"permalink":"https://example.org/quick-start/installation/","title":"Installation"},{"content":"Running ETLX The binary supports the following flags:\n--config: Path to the Markdown configuration file. (Default: config.md) --date: Reference date for the ETL process in YYYY-MM-DD format. (Default: yesterday\u0026rsquo;s date) --only: Comma-separated list of keys to run. --skip: Comma-separated list of keys to skip. --steps: Steps to run within the ETL process (extract, transform, load). --file: Path to a specific file to extract data from. Typically used with the --only flag. --clean: Execute clean_sql on items (conditional based on --only and --skip). --drop: Execute drop_sql on items (conditional based on --only and --skip). --rows: Retrieve the number of rows in the target table(s). 1 etlx --config etl_config.md --date 2023-10-31 --only sales --steps extract,load üê≥ Running ETLX with Docker You can run etlx directly from Docker without installing Go or building locally.\nBuild the Image Clone the repo and build:\n1 docker build -t etlx:latest . Or pull the prebuilt image (when published):\n1 docker pull docker.io/realdatadriven/etlx:latest Running Commands The image behaves exactly like the CLI binary. For example:\n1 2 3 docker run --rm etlx:latest help docker run --rm etlx:latest version docker run --rm etlx:latest run --config /app/config.md Using a .env File If you have a .env file with environment variables, mount it into /app/.env:\n1 2 3 docker run --rm \\ -v $(pwd)/.env:/app/.env:ro \\ etlx:latest run --config /app/config.md Mounting Config Files Mount your config file into the container and reference it by path:\n1 2 3 docker run --rm \\ -v $(pwd)/config.md:/app/config.md:ro \\ etlx:latest run --config /app/config.md Database Directory etlx can attach a database directory. Mount your local ./database directory into /app/database:\n1 2 3 docker run --rm \\ -v $(pwd)/database:/app/database \\ etlx:latest run --config /app/config.md Combine All Together Mount .env, config, and database directory:\n1 2 3 4 5 docker run --rm \\ -v $(pwd)/.env:/app/.env:ro \\ -v $(pwd)/config.md:/app/config.md:ro \\ -v $(pwd)/database:/app/database \\ etlx:latest run --config /app/config.md Interactive Mode For interactive subcommands (like repl):\n1 docker run -it --rm etlx:latest repl üí° Pro Tip: Local Alias You can add an alias so Docker feels like the native binary:\n1 alias etlx=\u0026#34;docker run --rm -v $(pwd):/app etlx:latest\u0026#34; Now you can just run:\n1 2 etlx help etlx run --config /app/config.md How It Works Create a Markdown file with the ETL process configuration. For example, see the example use case in the examples section.\n","description":null,"permalink":"https://example.org/quick-start/running/","title":"Running ETLX"},{"content":"Features Markdown-based configurations for easy readability. Extensive DuckDB support for various data sources and formats. Modular design for reusability and clarity. Built-in error handling and validation. Export functionality for reports, templates, and data lakes. ","description":null,"permalink":"https://example.org/features/","title":"Features"},{"content":"ETL Defines the overall ETL process. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # ETL ```yaml metadata name: Daily_ETL description: \u0026#39;Daily extraction at 5 AM\u0026#39; database: analytics_db connection: \u0026#39;postgres:user=@PGUSER password=@PGPASSWORD dbname=analytics_db host=localhost port=5432 sslmode=disable\u0026#39; ``` ## sales_data ```yaml metadata name: SalesData description: \u0026#39;Daily Sales Data\u0026#39; load_conn: \u0026#39;duckdb:\u0026#39; load_before_sql: - load_extentions - conn load_validation: # Validation is performed during the load phase using YAML - type: throw_if_empty sql: validate_data_not_empty msg: \u0026#39;No data extracted for the given date!\u0026#39; - type: throw_if_not_empty sql: validate_data_duplicates msg: \u0026#39;Duplicate data detected!\u0026#39; load_sql: load_sales load_after_sql: detaches ``` 1. ETL Process Starts Begin with the \u0026ldquo;ETL\u0026rdquo; key; Extract metadata, specifically: \u0026ldquo;connection\u0026rdquo;: Main connection to the destination database. \u0026ldquo;description\u0026rdquo;: For logging the start and end time of the ETL process. 2. Loop through Level 2 key in under \u0026ldquo;ETL\u0026rdquo; key Iterate over each key (e.g., \u0026ldquo;sales_data\u0026rdquo;) For each key, access its \u0026ldquo;metadata\u0026rdquo; to process the ETL steps. 3. ETL Steps Each ETL step (extract, transform, load) has: _before_sql: Queries to run first (setup). _sql: The main query or queries to run. _after_sql: Cleanup queries to run afterward. Queries can be: null: Do nothing. string: Reference a single query key in the same map or the query itself. array|list: Execute all queries in sequence. In case is not null it can be the query itself or just the name of a sql code block under the same key, where sql [query_name] or first line -- [query_name] Use _conn for connection settings. If null, fall back to the main connection. Additionally, error handling can be defined using [step]_on_err_match_patt and [step]_on_err_match_sql to handle specific database errors dynamically, where [step]_on_err_match_patt is the regexp patthern to match error,and if maches the [step]_on_err_match_sql is executed, the same can be applied for [step]_before_on_err_match_patt and [step]_before_on_err_match_sql. You can define patterns to match specific errors and provide SQL statements to resolve those errors. This feature is useful when working with dynamically created databases, tables, or schemas. 4. Output Logsn Log progress (e.g., connection usage, start/end times, descriptions). Gracefully handle missing or null keys. ","description":null,"permalink":"https://example.org/configuration/etl/","title":"ETL"},{"content":"Validation Rules Validate data quality during the ETL process by using the key [step]_validation in the metadata section. Example: 1 2 3 4 5 6 7 8 9 10 ... load_validation: - type: throw_if_empty sql: validate_data_not_empty msg: \u0026#39;No data extracted for the given date!\u0026#39; active: false - type: throw_if_not_empty sql: validate_data_duplicates msg: \u0026#39;Duplicate data detected!\u0026#39; ... For every object in the [step]_validation the sql is executed in the [step]_con connection, and it can either throw error message defined in msg or not if the condition (type:throw_if_empty | type:throw_if_not_empty or) is met or not.\nExtracting Data from Unsupported Databases If the database you are using does not have a direct DuckDB scanner, but it is supported by sqlx or it has odbc support, you must set the to_csv option to true in the extract configuration. This ensures that data is first exported to a CSV file and then on step load the file can be processed by DuckDB.\nExample Configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ... ## table_from_odbc_source ```yaml metadata name: table_from_odbc_source description: \u0026#39;This is an example o how to extract from databases that does not have a DuckDB scanner\u0026#39; to_csv: true extract_conn: \u0026#39;odbc:DRIVER={ODBC Driver 17 for SQL Server};SERVER=@MSSQL_HOST;UID=@MSSQL_USER;PWD=@MSSQL_PASS;DATABASE=DB\u0026#39; extract_sql: | SELECT [fields] FROM [table] WHERE [condition] load_conn: \u0026#39;duckdb:\u0026#39; load_before_sql: - load_extentions - conn load_sql: load_exported_csv load_after_sql: detaches ``` ... Once extracted, the CSV file can be loaded by DuckDB using: ```sql load_exported_csv CREATE OR REPLACE TABLE DB.target_table AS SELECT * FROM READ_CSV(\u0026#39;\u0026lt;fname\u0026gt;\u0026#39;, HEADER TRUE); ``` ##### **Why Use `to_csv: true`?** - **Workaround for unsupported databases**: If DuckDB does not have a direct scanner, exporting data to CSV allows and then the CSV can be loaded by the DuckDB. - **Ensures compatibility**: ETLX will handle the conversion, making the data accessible for further transformation and loading. - **Required for smooth ETL workflows**: Without this option, DuckDB may fail to recognize or query the database directly. ","description":null,"permalink":"https://example.org/configuration/validation/","title":"Validation Rules"},{"content":"Query Documentation In some ETL processes, particularly during the Transform step, queries may become too complex to manage as a single string. To address this, the configuration supports a structured approach where you can break down a query into individual fields and their respective SQL components. This approach improves modularity, readability, and maintainability.\nStructure A complex query is defined as a top-level heading (e.g., # My Complex Query) in the configuration. Each field included in the query is represented as a Level 2 heading (e.g., ## Field Name).\nFor each field:\nMetadata can describe the field (e.g., name, description) if a yaml metadata is not provided the field key is used as field in this example Field Name. SQL components like select, from, join, where, group_by, order_by, having, and cte are specified in separate sql blocks. Markdown Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # My Complex Query This query processes sales and regions data. ```yaml metadata name: sales_and_regions_query description: \u0026#34;Combines sales data with region metadata.\u0026#34; ``` ## Sales Field ```yaml metadata name: sales_field description: \u0026#34;Field representing sales data.\u0026#34; ``` ```sql -- select SELECT S.total_sales AS sales_field ``` ```sql -- from FROM sales_data AS S ``` ## Regions Field ```yaml metadata name: regions_field description: \u0026#34;Field representing region metadata.\u0026#34; ``` ```sql -- cte WITH region_cte AS ( SELECT region_id, region_name FROM region_data WHERE active = TRUE ) ``` ```sql -- select , R.region_name AS regions_field ``` ```sql -- join LEFT JOIN region_cte AS R ON S.region_id = R.region_id ``` ```sql -- where WHERE S.total_sales \u0026gt; 1000 ``` (See README for full explanation of how query doc parsing and combining works.)\n","description":null,"permalink":"https://example.org/configuration/query-doc/","title":"Query Documentation"},{"content":"Data Quality The DATA_QUALITY section allows you to define and execute validation rules to ensure the quality of your data. Each rule performs a check using a SQL query to identify records that violate a specific condition. Optionally, you can define a query to fix any identified issues automatically if applicable.\nData Quality Structure Metadata:\nThe DATA_QUALITY section contains metadata describing its purpose and activation status. Validation Rules:\nEach validation rule is defined as a Level 2 heading under the DATA_QUALITY block. Rules include a query to check for violations and, optionally, a query to fix issues. Execution:\nThe system loops through all rules in the DATA_QUALITY block. For each rule: Runs the validation query. If violations are found and a fix query is defined, executes the fix query. (See README for full examples and behavior details.)\n","description":null,"permalink":"https://example.org/configuration/data-quality/","title":"Data Quality"},{"content":"Exports The EXPORTS section in the ETL configuration handles exporting data to files. This is particularly useful for generating reports for internal departments, regulators, partners, or saving processed data to a data lake. By leveraging DuckDB\u0026rsquo;s ability to export data in various formats, this section supports file generation with flexibility and precision.\n(See README for full examples including CSV, Excel, and template-based exports.)\n","description":null,"permalink":"https://example.org/configuration/exports/","title":"Exports"},{"content":"Scripts The SCRIPTS section allows you to execute SQL queries that don‚Äôt fit into other predefined sections (ETL, EXPORTS, etc.).\nüîπ When to Use SCRIPTS? ‚úÖ Running cleanup queries after an ETL job\n‚úÖ Executing ad-hoc maintenance tasks\n‚úÖ Running SQL commands that don‚Äôt need to return results\n‚úÖ Executing SQL scripts for database optimizations\n(See README for full markdown examples and behavior.)\n","description":null,"permalink":"https://example.org/configuration/scripts/","title":"Scripts"},{"content":"Multi-Queries The MULTI_QUERIES section allows you to define multiple queries with similar structures and aggregate their results using a SQL UNION. This is particularly useful when generating summaries or reports that combine data from multiple queries into a single result set.\n(See README for full examples and execution flow.)\n","description":null,"permalink":"https://example.org/configuration/multi-queries/","title":"Multi-Queries"},{"content":"Logs Handling (# LOGS) ETLX provides a logging mechanism that allows saving logs into a database. This is useful for tracking executions, debugging, and auditing ETL processes.\n(See README for example configurations like AUTO_LOGS and create_missing_columns dynamic helper.)\n","description":null,"permalink":"https://example.org/configuration/logs/","title":"Logs"},{"content":"REQUIRES The REQUIRES section in the ETL configuration allows you to load dependencies from external Markdown configurations. These dependencies can either be loaded from file paths or dynamically through queries. This feature promotes modularity and reusability by enabling you to define reusable parts of the configuration in separate files or queries.\n(See README for full examples showing loading from queries and files.)\n","description":null,"permalink":"https://example.org/configuration/requires/","title":"Loading Config Dependencies"},{"content":"ACTIONS There are scenarios in ETL workflows where actions such as downloading, uploading, compressing or copying files cannot be performed using SQL alone. The ACTIONS section allows you to define steps for copying or transferring files using the file system or external protocols.\n(See README for the full list of supported action types and examples.)\n","description":null,"permalink":"https://example.org/configuration/actions/","title":"Actions"},{"content":"NOTIFY The NOTIFY section enables sending notifications (e.g., email via SMTP) with dynamic templates populated from SQL query results. This is useful for monitoring ETL processes and sending status reports.\n(See README for full markdown example including template and data_sql examples.)\n","description":null,"permalink":"https://example.org/configuration/notify/","title":"Notify"},{"content":"Dynamic Query Generation (get_dyn_queries[...]) In some advanced ETL workflows, you may need to dynamically generate SQL queries based on metadata or schema differences between the source and destination databases.\n(See README for examples and the create_missing_columns query example.)\n","description":null,"permalink":"https://example.org/advanced/dynamic-queries/","title":"Dynamic Queries"},{"content":"Conditional Execution ETLX allows conditional execution of SQL blocks based on the results of a query. This is useful to skip operations dynamically depending on data context (e.g., skip a step if no new data is available).\n(See README for examples using *_condition and condition keys.)\n","description":null,"permalink":"https://example.org/advanced/conditional-execution/","title":"Conditional Execution"},{"content":"Advanced Workflow Execution: runs_as Override The runs_as field in the metadata block of any Level 1 key allows ETLX to treat a custom section as a built-in block (like ETL, EXPORTS, etc.), enabling advanced chaining of processes within the same configuration.\n(See README for examples.)\n","description":null,"permalink":"https://example.org/advanced/runs-as/","title":"runs_as override"},{"content":"Embedding in Go To embed the ETL framework in a Go application, you can use the etlx package and call ConfigFromMDText and RunETL. Example (from README):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/realdatadriven/etlx\u0026#34; ) func main() { etl := \u0026amp;etlx.ETLX{} // Load configuration from Markdown text err := etl.ConfigFromMDText(`# Your Markdown config here`) if err != nil { fmt.Printf(\u0026#34;Error loading config: %v\\n\u0026#34;, err) return } // Prepare date reference dateRef := []time.Time{time.Now().AddDate(0, 0, -1)} // Define additional options options := map[string]any{ \u0026#34;only\u0026#34;: []string{\u0026#34;sales\u0026#34;}, \u0026#34;steps\u0026#34;: []string{\u0026#34;extract\u0026#34;, \u0026#34;load\u0026#34;}, } // Run ETL process logs, err := etl.RunETL(dateRef, nil, options) if err != nil { fmt.Printf(\u0026#34;Error running ETL: %v\\n\u0026#34;, err) return } // Print logs for _, log := range logs { fmt.Printf(\u0026#34;Log: %+v\\n\u0026#34;, log) } } ","description":null,"permalink":"https://example.org/embedding/","title":"Embedding in Go"},{"content":"Future Plans ETLX is a powerful tool for defining and executing ETL processes using Markdown configuration files. It supports complex SQL queries, exports to multiple formats, and dynamic configuration loading. ETLX can be used as a library, CLI tool, or integrated into other systems for advanced data workflows.\nTo-Do List ‚úÖ Completed Config Parsing:\nParses and validates Markdown configurations with nested sections and metadata. Supports YAML, TOML, and JSON for metadata. ETL Execution:\nModular handling of extract, transform, and load processes. Flexible step configuration with before and after SQL. Query Documentation:\nHandles complex SQL queries by breaking them into logical components. Dynamically combines query parts to create executable SQL. Exports:\nSupports exporting data to files in formats like CSV and Excel. Includes options for templates and data mapping. Requires:\nLoads additional configurations dynamically from files or database queries. Integrates loaded configurations into the main process. CLI Interface:\nProvides a command-line interface for running configurations. Accepts flags for custom execution parameters. üïí To-Do Web API:\nCreate a RESTful web API for executing ETL configurations. Expose endpoints for: Uploading and managing configurations. Triggering ETL workflows. Monitoring job status and logs. Add support for multi-user environments with authentication and authorization. ","description":null,"permalink":"https://example.org/future/","title":"Future Plans"},{"content":"License This project is licensed under the MIT License.\n","description":null,"permalink":"https://example.org/license/","title":"License"},{"content":"Installation (new site) Create a new Hugo site with Plume:\nhugo new site mysite cd mysite hugo mod init github.com/\u0026lt;username\u0026gt;/\u0026lt;site\u0026gt; hugo mod tidy hugo mod get github.com/Dobefu/hugo-theme-plume Add to your hugo.toml:\n1 2 3 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; Installation (existing site) As a Hugo Module hugo mod init github.com/\u0026lt;username\u0026gt;/\u0026lt;site\u0026gt; hugo mod tidy hugo mod get github.com/Dobefu/hugo-theme-plume Add to your hugo.toml:\n1 2 3 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; As a Git Submodule git submodule add https://github.com/Dobefu/hugo-theme-plume.git themes/plume Add to your hugo.toml:\n1 theme = \u0026#34;plume\u0026#34; Configuration To configure the theme, update the hugo.toml file with the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [module] [[module.imports]] path = \u0026#34;github.com/Dobefu/hugo-theme-plume\u0026#34; [markup] [markup.tableOfContents] startLevel = 2 endLevel = 4 [markup.highlight] lineNos = true noClasses = false [markup.goldmark] [markup.goldmark.extensions] strikethrough = false [markup.goldmark.extensions.extras] delete.enable = true mark.enable = true insert.enable = true subscript.enable = true superscript.enable = true [markup.goldmark.extensions.passthrough] enable = true [markup.goldmark.extensions.passthrough.delimiters] block = [[\u0026#39;\\[\u0026#39;, \u0026#39;\\]\u0026#39;]] inline = [[\u0026#39;\\(\u0026#39;, \u0026#39;\\)\u0026#39;]] ","description":"Installation instructions for the Plume theme.","permalink":"https://example.org/docs/getting-started/installation/","title":"Installation"}]